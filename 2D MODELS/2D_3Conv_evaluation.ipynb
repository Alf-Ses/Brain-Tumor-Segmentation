{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMqGqyZQNPOzk92UqEh+Jii"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oO6FC3e-QlqG","executionInfo":{"status":"ok","timestamp":1720430714647,"user_tz":-120,"elapsed":91683,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}},"outputId":"262b05b9-2c5d-4793-9a97-222ca6c0ebf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.16.0rc0\n","  Downloading tensorflow-2.16.0rc0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m804.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (0.2.0)\n","Collecting h5py>=3.10.0 (from tensorflow==2.16.0rc0)\n","  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (18.1.1)\n","Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.0rc0)\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (2.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (1.64.1)\n","Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.0rc0)\n","  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras>=3.0.0 (from tensorflow==2.16.0rc0)\n","  Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (0.37.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.16.0rc0) (1.25.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.0rc0) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow==2.16.0rc0) (13.7.1)\n","Collecting namex (from keras>=3.0.0->tensorflow==2.16.0rc0)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Collecting optree (from keras>=3.0.0->tensorflow==2.16.0rc0)\n","  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.0rc0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.0rc0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.0rc0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.0rc0) (2024.6.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.0rc0) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.0rc0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.0rc0) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.0rc0) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.0rc0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.0rc0) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.0rc0) (0.1.2)\n","Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.9.0\n","    Uninstalling h5py-3.9.0:\n","      Successfully uninstalled h5py-3.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.0rc0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.4.1 ml-dtypes-0.3.2 namex-0.0.8 optree-0.12.1 tensorboard-2.16.2 tensorflow-2.16.0rc0\n","Collecting patchify\n","  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from patchify) (1.25.2)\n","Installing collected packages: patchify\n","Successfully installed patchify-0.2.3\n"]}],"source":["!pip install tensorflow==2.16.0rc0\n","!PYTHONHASHSEED=0\n","!pip install patchify"]},{"cell_type":"code","source":["# Import other modules\n","from matplotlib import pyplot as plt\n","import zipfile\n","from shutil import copyfile\n","from time import time\n","import numpy as np\n","import pandas as pd\n","import random as python_random\n","import os\n","import shutil\n","import glob\n","from patchify import patchify, unpatchify\n","import random\n","\n","# Import TensorFlow/Keras\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv3D, Conv3DTranspose, MaxPooling3D, concatenate, Dropout, Activation, BatchNormalization, GroupNormalization\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import tensorflow.keras.backend as K\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W17oitOgQuUe","executionInfo":{"status":"ok","timestamp":1720430736627,"user_tz":-120,"elapsed":21984,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}},"outputId":"73096f3c-2351-4d8b-8b9e-16530aaf84a4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Percorso del file ZIP su Google Drive\n","drive_zip_path = '/content/drive/MyDrive/DL_PROJECT/Data/Test_128.zip'\n","local_extract_path = '/content/Test_Data'\n","os.makedirs(local_extract_path, exist_ok=True)\n","\n","# Decomprimere il file ZIP nella directory locale\n","with zipfile.ZipFile(drive_zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(local_extract_path)\n","\n","print(\"Decompressione completata.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3AUf6vFRBc6","executionInfo":{"status":"ok","timestamp":1720430859937,"user_tz":-120,"elapsed":81624,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}},"outputId":"dc8ad9c5-4943-4872-e1ec-2795c4b44baf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Decompressione completata.\n"]}]},{"cell_type":"code","source":["import numpy as np  # Import the numpy library for array manipulation\n","\n","# Function to load images from a given directory and list of image file names\n","def load_img(img_dir, img_list):\n","    images = []  # Initialize an empty list to store the images\n","    for image_name in img_list:  # Loop through each image name in the list\n","        if image_name.split('.')[-1] == 'npy':  # Check if the file is a .npy file\n","            volume = np.load(img_dir + image_name)  # Load the .npy file as a numpy array\n","            # Extract 128 images along the third axis (z-axis) from each volume\n","            for j in range(volume.shape[2]):\n","                images.append(volume[:, :, j, :])  # Append each slice to the images list\n","    images = np.array(images)  # Convert the list of images to a numpy array\n","    return images  # Return the numpy array of images\n","\n","# Generator function to load and yield image and mask batches\n","def image_loader(img_dir, img_list, mask_dir, mask_list, batch_size=256):\n","    assert batch_size == 256, \"Batch size must be 256 to match the number of slices per two volumes.\"  # Ensure batch size is 256\n","\n","    L = len(img_list)  # Get the length of the image list\n","\n","    while True:  # Infinite loop to keep yielding batches\n","        for i in range(0, L, 2):  # Iterate through the image list in steps of 2\n","            if i + 1 < L:  # Check if there are at least two more volumes to load\n","                img = load_img(img_dir, [img_list[i], img_list[i+1]])  # Load images from two volumes\n","                mask = load_img(mask_dir, [mask_list[i], mask_list[i+1]])  # Load corresponding masks\n","\n","                # Combine images and masks into a list of tuples and shuffle them\n","                combined = list(zip(img, mask))\n","                np.random.shuffle(combined)\n","                img[:], mask[:] = zip(*combined)  # Unzip the shuffled list back into images and masks\n","\n","                yield (img, mask)  # Yield the shuffled images and masks as a batch\n","            else:\n","                # Handle the case where there is an odd number of volumes\n","                img = load_img(img_dir, [img_list[i]])  # Load images from the last volume\n","                mask = load_img(mask_dir, [mask_list[i]])  # Load corresponding masks\n","\n","                # Combine images and masks into a list of tuples and shuffle them\n","                combined = list(zip(img, mask))\n","                np.random.shuffle(combined)\n","                img[:], mask[:] = zip(*combined)  # Unzip the shuffled list back into images and masks\n","\n","                yield (img, mask)  # Yield the shuffled images and masks as a batch"],"metadata":{"id":"1FlyiUxCRI9Y","executionInfo":{"status":"ok","timestamp":1720430859938,"user_tz":-120,"elapsed":4,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import tensorflow.keras.backend as K\n","\n","def DiceCoefficient(y_true, y_pred, smooth = 1e-6):\n","\n","    y_pred = tf.keras.activations.softmax(y_pred, axis = -1)\n","\n","    # Cast to float32 datatype\n","    y_true = K.cast(y_true, 'float32')\n","    y_pred = K.cast(y_pred, 'float32')\n","\n","    # Flatten label and prediction tensors\n","    inputs = K.flatten(y_pred)\n","    targets = K.flatten(y_true)\n","\n","    intersection = K.sum(inputs * targets)\n","    dice = (2 * intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    return dice"],"metadata":{"id":"5UqRt79JQvLK","executionInfo":{"status":"ok","timestamp":1720430882035,"user_tz":-120,"elapsed":265,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model = keras.saving.load_model('/content/drive/MyDrive/DL_PROJECT/2D_FINAL/2D_Unet_final_2.keras', compile = True , custom_objects={'DiceCoefficient' : DiceCoefficient})"],"metadata":{"id":"GFfxTdUeQzWx","executionInfo":{"status":"ok","timestamp":1720430887513,"user_tz":-120,"elapsed":2872,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["test_img_dir = '/content/Test_Data/X_test/'\n","test_mask_dir = '/content/Test_Data/Y_test/'\n","\n","test_img_list = sorted(os.listdir(test_img_dir))\n","test_mask_list = sorted(os.listdir(test_mask_dir))"],"metadata":{"id":"SDOfub_fTnGu","executionInfo":{"status":"ok","timestamp":1720430900725,"user_tz":-120,"elapsed":246,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["batch_size = 256\n","\n","test_img_datagen = image_loader(test_img_dir, test_img_list, test_mask_dir, test_mask_list, batch_size)\n","\n","steps_per_epoch = len(test_img_list)//2\n","print(steps_per_epoch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcUXNFGPTm3n","executionInfo":{"status":"ok","timestamp":1720430903500,"user_tz":-120,"elapsed":229,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}},"outputId":"257e9bdc-d451-41c2-a134-eb03c5a4d32a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["36\n"]}]},{"cell_type":"code","source":["evaluation = model.evaluate(\n","    test_img_datagen,\n","    verbose = 1,\n","    steps = steps_per_epoch,\n","    callbacks = None,\n","    return_dict = True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53eCrNERTRrK","executionInfo":{"status":"ok","timestamp":1720432377323,"user_tz":-120,"elapsed":1472257,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}},"outputId":"344df918-604f-4533-9848-bc877f5734a8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1468s\u001b[0m 41s/step - accuracy: 0.9087 - dice_coefficient: 0.4133 - loss: 0.2906 - one_hot_io_u: 0.9149 - one_hot_io_u_1: 0.2796 - one_hot_io_u_2: 0.3091 - one_hot_io_u_3: 0.2961 - one_hot_mean_io_u: 0.4499\n"]}]},{"cell_type":"code","source":["evaluation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wQ5wA6bbwuG0","executionInfo":{"status":"ok","timestamp":1720432532868,"user_tz":-120,"elapsed":247,"user":{"displayName":"Alfredo Sestito","userId":"04562609772688808489"}},"outputId":"994cc1ca-31b1-482c-db54-596d37b7a3c7"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.929600179195404,\n"," 'dice_coefficient': 0.42059606313705444,\n"," 'loss': 0.30292290449142456,\n"," 'one_hot_io_u': 0.9361362457275391,\n"," 'one_hot_io_u_1': 0.32653361558914185,\n"," 'one_hot_io_u_2': 0.24520477652549744,\n"," 'one_hot_io_u_3': 0.33646467328071594,\n"," 'one_hot_mean_io_u': 0.46108490228652954}"]},"metadata":{},"execution_count":13}]}]}